{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Activation\n",
    "\n",
    "from skimage import io, transform, exposure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSVs Tran and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train = pd.read_csv(\"../dataset/archive/Train.csv\")[[\"ClassId\", \"Path\"]]\n",
    "df_Test = pd.read_csv(\"../dataset/archive/Test.csv\")[[\"ClassId\", \"Path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassId</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00004.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39204</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00025.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39205</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00026.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39206</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00027.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39207</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00028.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39208</td>\n",
       "      <td>42</td>\n",
       "      <td>Train/42/00042_00007_00029.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39209 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ClassId                            Path\n",
       "0           20  Train/20/00020_00000_00000.png\n",
       "1           20  Train/20/00020_00000_00001.png\n",
       "2           20  Train/20/00020_00000_00002.png\n",
       "3           20  Train/20/00020_00000_00003.png\n",
       "4           20  Train/20/00020_00000_00004.png\n",
       "...        ...                             ...\n",
       "39204       42  Train/42/00042_00007_00025.png\n",
       "39205       42  Train/42/00042_00007_00026.png\n",
       "39206       42  Train/42/00042_00007_00027.png\n",
       "39207       42  Train/42/00042_00007_00028.png\n",
       "39208       42  Train/42/00042_00007_00029.png\n",
       "\n",
       "[39209 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 39209 images (75.63610409151411)\n",
      "Test Size: 12630 images (24.363895908485887)\n"
     ]
    }
   ],
   "source": [
    "total = df_Train.shape[0]+df_Test.shape[0]\n",
    "print(f\"Train Size: {df_Train.shape[0]} images ({df_Train.shape[0]/ total*100})\")\n",
    "print(f\"Test Size: {df_Test.shape[0]} images ({df_Test.shape[0]/ total*100})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Load Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function first shuffle the dataset, after load the image to memory, resize it, equelize the contrast and finally save to an array.\n",
    "def load_images(main_path, df):\n",
    "    data = []\n",
    "    \n",
    "    # SHUFFLE DATA FRAME\n",
    "    df_shuffle = df.sample(frac=1)\n",
    "    \n",
    "    for (i, image_path) in enumerate(df):\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(f\"[DEBUG] Total processed images: {i}\")\n",
    "            \n",
    "        image_full_path = os.path.sep.join([main_path, image_path])\n",
    "        image = io.imread(image_full_path)\n",
    "\n",
    "        image = transform.resize(image, (32,32))\n",
    "        image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "\n",
    "        data.append(image)\n",
    "            \n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(width, height, depth, classes, conv_size, dense_size, filter_size):\n",
    "        model = Sequential()\n",
    "\n",
    "        inputShape = (height, width, depth)\n",
    "        dim = 1\n",
    "\n",
    "        model.add(Conv2D(filter_size[0], conv_size[0], padding = \"same\", input_shape = inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=dim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model.add(Conv2D(filter_size[1], conv_size[1], padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=dim))\n",
    "        model.add(Conv2D(filter_size[1], conv_size[1], padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=dim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model.add(Conv2D(filter_size[2], conv_size[2], padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=dim))\n",
    "        model.add(Conv2D(filter_size[2], conv_size[2], padding = \"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=dim))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_size[0]))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_size[1]))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, path):\n",
    "    # GETTING THE CURRENT TIME\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    \n",
    "    # SAVING MODEL\n",
    "    model_path = path + date\n",
    "    print(f\"[INFO] serialiazing network to {model_path}\")\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printPrediction(model, Y_Test, prediction, label_names, file_name):\n",
    "    pred = classification_report(Y_Test.argmax(axis=1), prediction.argmax(axis=1), target_names = label_names, output_dict=True)\n",
    "#     print(pred)\n",
    "    \n",
    "    df_pred = pd.DataFrame(pred).transpose()\n",
    "    df_pred.to_csv(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePlotMetrics(num_epochs, hist, path, title):\n",
    "    N = np.arange(0, num_epochs)\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(N, hist.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(N, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(N, hist.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(N, hist.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(f\"Training Loss and Accuracy on Database - {title}.\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveHist(hist, file_name):\n",
    "    df_hist = pd.DataFrame(hist)\n",
    "    df_hist.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    NUM_EPOCHS = 30\n",
    "    INIT_LR = 0.001\n",
    "    BS = 64\n",
    "\n",
    "    print(f\"[DEBUG] Loading dataset ...\\n\")\n",
    "    df_Train = pd.read_csv(\"../dataset/archive/Train.csv\")[[\"ClassId\", \"Path\"]]\n",
    "    df_Test = pd.read_csv(\"../dataset/archive/Test.csv\")[[\"ClassId\", \"Path\"]]\n",
    "    label_names = pd.read_csv(\"../dataset/archive/signnames.csv\")[\"SignName\"]\n",
    "\n",
    "    print(f\"[DEBUG] Load images from file and prepare data ...\\n\")\n",
    "    X_Train = load_images(\"../dataset/archive\", df_Train[\"Path\"])\n",
    "    X_Test = load_images(\"../dataset/archive\", df_Test[\"Path\"])\n",
    "\n",
    "    # GET THE TOTAL UNIQUE TRANSIT SIGN\n",
    "    num_labels = df_Train[\"ClassId\"].unique().size\n",
    "\n",
    "    # GET THE Y CLASS\n",
    "    Y_Train = np.array(df_Train[\"ClassId\"])\n",
    "    Y_Test = np.array(df_Test[\"ClassId\"])\n",
    "\n",
    "    # NORMALIZE THE RGB PIXELS VALUES\n",
    "    X_Train = X_Train.astype(\"float32\") / 255.0\n",
    "    X_Test  = X_Test.astype(\"float32\") / 255.0\n",
    "\n",
    "    # ONE HOT ENCODING\n",
    "    Y_Train = to_categorical(Y_Train, num_labels)\n",
    "    Y_Test = to_categorical(Y_Test, num_labels)  \n",
    "\n",
    "    # CREATE A DICTIONARY BY CLASS AND CALC THE WEIGHT, THIS BECAUSE THE DATASET IS NOT HOMOGENEOUS\n",
    "    # GET THE TOTAL TRANSIT SIGN IMAGES\n",
    "    total_classes = Y_Train.sum(axis=0)\n",
    "    class_weight = dict()\n",
    "\n",
    "    # CALCULATE THE CLASS WEIGHT\n",
    "    for i in range(0, len(total_classes)):\n",
    "        class_weight[i] = total_classes.max() / total_classes[i]\n",
    "\n",
    "\n",
    "    conv_size   = [[(2,2), (3,3), (3,3)], [(3,3), (3,3), (3,3)] , [(5,5), (3,3), (3,3)], [(5,5), (5,5), (5,5)], [(5,5), (8,8), (4,4)]]\n",
    "    dense_size  = [[64, 64], [64, 128], [128, 256], [256, 256], [256, 512]]\n",
    "    filter_size = [[4,4,8], [4,8,16], [8,16,32], [16,32,64], [32,32,64]]\n",
    "\n",
    "    for i in range(0, len(conv_size)):\n",
    "        # IMAGE GENERATOR\n",
    "        img_gen = ImageDataGenerator(rotation_range = 10,\n",
    "                                     zoom_range = 0.15,\n",
    "                                     width_shift_range = 0.1,\n",
    "                                     height_shift_range = 0.1,\n",
    "                                     shear_range = 0.15,\n",
    "                                     horizontal_flip = False,\n",
    "                                     vertical_flip = False,\n",
    "                                     fill_mode = \"nearest\")\n",
    "\n",
    "        print(f\"[DEBUG] Config Model ...\\n\")\n",
    "        # ADAM OPTIMIZER\n",
    "        opt = Adam(learning_rate=INIT_LR, decay = INIT_LR / (NUM_EPOCHS * 0.5))                \n",
    "\n",
    "        # CNN\n",
    "        model = build(width = 32, height = 32, depth = 3, classes = num_labels, conv_size = conv_size[i], dense_size = dense_size[i], filter_size = filter_size[i])\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "\n",
    "        print(f\"[DEBUG] Training ...\\n\")\n",
    "\n",
    "        hist = model.fit(img_gen.flow(X_Train, Y_Train, batch_size = BS),\n",
    "                        validation_data = (X_Test, Y_Test), \n",
    "                        steps_per_epoch = X_Train.shape[0] // BS,\n",
    "                        epochs = NUM_EPOCHS,\n",
    "                        class_weight = class_weight,\n",
    "                        verbose = 1)\n",
    "\n",
    "        print(f\"[DEBUG] Evaluating ...\\n\")\n",
    "        prediction = model.predict(X_Test, batch_size = BS)\n",
    "\n",
    "        conv_name = f\"_Conv_{conv_size[i][0][0]}x{conv_size[i][0][1]}_{conv_size[i][1][0]}x{conv_size[i][1][1]}_{conv_size[i][2][0]}x{conv_size[i][2][1]}\"\n",
    "        dense_name = f\"_Dense_{dense_size[i][0]}_{dense_size[i][1]}\"\n",
    "        filter_name = f\"_Filter_{filter_size[i][0]}_{filter_size[i][1]}_{filter_size[i][2]}\"\n",
    "\n",
    "        model_name = conv_name+dense_name+filter_name\n",
    "\n",
    "        print(f\"[DEBUG] Saving Prediction ...\\n\")\n",
    "        printPrediction(model, Y_Test, prediction, label_names, f\"../plots/pred_{model_name}.csv\")\n",
    "        print(f\"[DEBUG] Saving Model ...\\n\")\n",
    "        saveModel(model, f\"../models/sign_model_{model_name}.h5\")\n",
    "        print(f\"[DEBUG] Saving plot ...\\n\")        \n",
    "        savePlotMetrics(NUM_EPOCHS, hist, f\"../plots/plot_{model_name}.png\", model_name)\n",
    "        print(f\"[DEBUG] Saving hist ...\\n\")          \n",
    "        saveHist(hist,f\"../hists/hist_{model_name}.csv\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Config Model ...\n",
      "\n",
      "[DEBUG] Training ...\n",
      "\n",
      "Epoch 1/2\n",
      "260/612 [===========>..................] - ETA: 25s - loss: 9.9461 - accuracy: 0.0640"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-64f062c60701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     verbose = 1)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[DEBUG] Evaluating ...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,len(conv_size)):\n",
    "\n",
    "    # IMAGE GENERATOR\n",
    "    img_gen = ImageDataGenerator(rotation_range = 10,\n",
    "                                 zoom_range = 0.15,\n",
    "                                 width_shift_range = 0.1,\n",
    "                                 height_shift_range = 0.1,\n",
    "                                 shear_range = 0.15,\n",
    "                                 horizontal_flip = False,\n",
    "                                 vertical_flip = False,\n",
    "                                 fill_mode = \"nearest\")\n",
    "\n",
    "    print(f\"[DEBUG] Config Model ...\\n\")\n",
    "    # ADAM OPTIMIZER\n",
    "    opt = Adam(learning_rate=INIT_LR, decay = INIT_LR / (NUM_EPOCHS * 0.5))                \n",
    "\n",
    "    # CNN\n",
    "    model = build(width = 32, height = 32, depth = 3, classes = num_labels, conv_size = conv, dense_size = dense, filter_size = filter_)\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "\n",
    "    print(f\"[DEBUG] Training ...\\n\")\n",
    "\n",
    "    hist = model.fit(img_gen.flow(X_Train, Y_Train, batch_size = BS),\n",
    "                    validation_data = (X_Test, Y_Test), \n",
    "                    steps_per_epoch = X_Train.shape[0] // BS,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    class_weight = class_weight,\n",
    "                    verbose = 1)\n",
    "\n",
    "    print(f\"[DEBUG] Evaluating ...\\n\")\n",
    "    prediction = model.predict(X_Test, batch_size = BS)\n",
    "\n",
    "    conv_name = f\"_Conv_{conv_size[i][0][0]}x{conv_size[i][0][1]}_{conv_size[i][1][0]}x{conv_size[i][1][1]}_{conv_size[i][2][0]}x{conv_size[i][2][1]}\"\n",
    "    dense_name = f\"_Dense_{dense_size[i][0]}_{dense_size[i][1]}\"\n",
    "    filter_name = f\"_Filter_{filter_size[i][0]}_{filter_size[i][1]}_{filter_size[i][2]}\"\n",
    "\n",
    "    file_name = conv_name+dense_name+filter_name\n",
    "    \n",
    "    print(f\"[DEBUG] Saving Prediction ...\\n\")\n",
    "    printPrediction(model, Y_Test, prediction, label_names, f\"../plots/pred_{model_name}.csv\")\n",
    "    print(f\"[DEBUG] Saving Model ...\\n\")\n",
    "    saveModel(model, f\"../models/sign_model_{model_name}.h5\")\n",
    "    print(f\"[DEBUG] Saving plot ...\\n\")        \n",
    "    savePlotMetrics(NUM_EPOCHS, hist, f\"../plots/plot_{model_name}.png\", model_name)\n",
    "    print(f\"[DEBUG] Saving hist ...\\n\")          \n",
    "    saveHist(hist,f\"../hists/hist_{model_name}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.184131</td>\n",
       "      <td>0.082003</td>\n",
       "      <td>7.979764</td>\n",
       "      <td>0.046397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.623126</td>\n",
       "      <td>0.193205</td>\n",
       "      <td>2.852304</td>\n",
       "      <td>0.276801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  9.184131  0.082003  7.979764      0.046397\n",
       "1  6.623126  0.193205  2.852304      0.276801"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
